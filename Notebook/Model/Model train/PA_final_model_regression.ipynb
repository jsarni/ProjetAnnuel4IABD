{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merzo\\Anaconda3\\envs\\tensorflow2gpu\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.core.computation.expressions import evaluate\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import normalize\n",
    "from tensorflow.keras.activations import *\n",
    "from tensorflow.keras.datasets import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.externals import joblib\n",
    "import os \n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_batch_path = 'D:\\\\BUREAU\\\\corsESGI\\\\Projet_annuel\\\\Dataset_train_test_parts\\\\Batch_dataset\\\\'\n",
    "train_path= 'D:\\\\BUREAU\\\\corsESGI\\\\Projet_annuel\\\\pickeled\\\\Train\\\\'\n",
    "test_path= 'D:\\\\BUREAU\\\\corsESGI\\\\Projet_annuel\\\\pickeled\\\\test\\\\'\n",
    "train = 'Train\\\\'\n",
    "test = 'Test\\\\'\n",
    "target_path = 'Target\\\\'\n",
    "X_data_path = 'X_data\\\\'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation function model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def baseline_model(loss,optimizer,metrics):\n",
    " # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(52, input_dim=52, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    model.add(Dense(52, kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(52, kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(52, kernel_initializer='normal',activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "    # Compile model\n",
    "    model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss='mse'\n",
    "optimizer='adam'\n",
    "optimizer_juba = Adam(learning_rate=0.5)\n",
    "metrics=['mse','mae']\n",
    "model = baseline_model(loss,optimizer,metrics)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MinMaxScaler for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "def get_normalize():\n",
    "    x_train_list =[f for f in listdir( train_path+X_data_path) if isfile(join(train_path+X_data_path, f))]\n",
    "    y_train_list = [f for f in listdir( train_path+target_path) if isfile(join(train_path+target_path, f))]\n",
    "    \n",
    "    X_train = pd.read_csv(train_path+X_data_path+x_train_list[0],sep=';').values\n",
    "    Y_train = pd.read_csv(train_path+target_path+y_train_list[0],sep=';',header = None).values\n",
    "    \n",
    "    \n",
    "    y= Y_train\n",
    "    x = X_train\n",
    "\n",
    "    y = np.reshape(y, (-1,1))\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    print(scaler_x.fit(x))\n",
    "    print(scaler_y.fit(y))\n",
    "    \n",
    "    # save MinMaxScaler \n",
    "    y_scaler_filename = \"y_scaler.save\"\n",
    "    x_scaler_filename = \"x_scaler.save\"\n",
    "    \n",
    "    joblib.dump(scaler_x, x_scaler_filename,protocol=0) \n",
    "    joblib.dump(scaler_y, y_scaler_filename,protocol=0) \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return (scaler_x,scaler_y)\n",
    "(scaler_x,scaler_y)= get_normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tain_model(model,epochs,batch_size,scaler_x,scaler_y):\n",
    "    x_train_list =[f for f in listdir( train_path+X_data_path) if isfile(join(train_path+X_data_path, f))]\n",
    "    y_train_list = [f for f in listdir( train_path+target_path) if isfile(join(train_path+target_path, f))]\n",
    "    x_test_list = [f for f in listdir( test_path+X_data_path) if isfile(join(test_path+X_data_path, f))]\n",
    "    y_test_list =  [f for f in listdir( test_path+target_path) if isfile(join(test_path+target_path, f))]\n",
    "    \n",
    "    X_train = pd.read_csv(train_path+X_data_path+x_train_list[0],sep=';').values\n",
    "    Y_train = pd.read_csv(train_path+target_path+y_train_list[0],sep=';',header = None).values\n",
    "\n",
    "    X_test = pd.read_csv(test_path+X_data_path+x_test_list[0],sep=';').values\n",
    "    Y_test = pd.read_csv(test_path+target_path+y_test_list[0],sep=';',header = None).values\n",
    "\n",
    "    scaler_x.fit(X_train)\n",
    "    xscale=scaler_x.transform(X_train)\n",
    "    scaler_y.fit(Y_train)\n",
    "    yscale=scaler_y.transform(Y_train)\n",
    "\n",
    "\n",
    "    scaler_x.fit(X_test)\n",
    "    xscale_test=scaler_x.transform(X_test)\n",
    "    scaler_y.fit(Y_test)\n",
    "    yscale_test=scaler_y.transform(Y_test)\n",
    "\n",
    "\n",
    "    model.fit(xscale,yscale, epochs=epochs, batch_size=batch_size, validation_data= (xscale_test,yscale_test),verbose=1)\n",
    "    del X_train\n",
    "    del Y_train\n",
    "    del X_test\n",
    "    del Y_test\n",
    "    del xscale\n",
    "    del yscale\n",
    "    del xscale_test\n",
    "    del yscale_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19853 samples, validate on 993302 samples\n",
      "Epoch 1/70\n",
      "19853/19853 [==============================] - 4s 225us/sample - loss: 5097.9751 - mse: 5097.9751 - mae: 62.3750 - val_loss: 5708.5749 - val_mse: 5708.5747 - val_mae: 49.6475\n",
      "Epoch 2/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 5096.0659 - mse: 5096.0659 - mae: 62.3598 - val_loss: 5706.9741 - val_mse: 5706.9736 - val_mae: 49.6317\n",
      "Epoch 3/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 5094.0493 - mse: 5094.0493 - mae: 62.3436 - val_loss: 5705.1303 - val_mse: 5705.1299 - val_mae: 49.6135\n",
      "Epoch 4/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 5091.7280 - mse: 5091.7280 - mae: 62.3250 - val_loss: 5703.0482 - val_mse: 5703.0479 - val_mae: 49.5929\n",
      "Epoch 5/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 5089.1143 - mse: 5089.1143 - mae: 62.3041 - val_loss: 5700.6690 - val_mse: 5700.6694 - val_mae: 49.5695\n",
      "Epoch 6/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 5086.1333 - mse: 5086.1333 - mae: 62.2801 - val_loss: 5697.9515 - val_mse: 5697.9512 - val_mae: 49.5426\n",
      "Epoch 7/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 5082.7197 - mse: 5082.7197 - mae: 62.2527 - val_loss: 5694.8738 - val_mse: 5694.8745 - val_mae: 49.5123\n",
      "Epoch 8/70\n",
      "19853/19853 [==============================] - 1s 44us/sample - loss: 5078.8696 - mse: 5078.8696 - mae: 62.2217 - val_loss: 5691.4134 - val_mse: 5691.4136 - val_mae: 49.4781\n",
      "Epoch 9/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 5074.5283 - mse: 5074.5283 - mae: 62.1868 - val_loss: 5687.3908 - val_mse: 5687.3911 - val_mae: 49.4384\n",
      "Epoch 10/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 5069.4766 - mse: 5069.4766 - mae: 62.1462 - val_loss: 5682.7262 - val_mse: 5682.7261 - val_mae: 49.3922\n",
      "Epoch 11/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 5063.6152 - mse: 5063.6152 - mae: 62.0990 - val_loss: 5677.3565 - val_mse: 5677.3564 - val_mae: 49.3390\n",
      "Epoch 12/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 5056.8657 - mse: 5056.8657 - mae: 62.0447 - val_loss: 5671.2044 - val_mse: 5671.2041 - val_mae: 49.2779\n",
      "Epoch 13/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 5049.1294 - mse: 5049.1294 - mae: 61.9823 - val_loss: 5664.1652 - val_mse: 5664.1650 - val_mae: 49.2080\n",
      "Epoch 14/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 5040.2798 - mse: 5040.2798 - mae: 61.9109 - val_loss: 5656.0514 - val_mse: 5656.0503 - val_mae: 49.1273\n",
      "Epoch 15/70\n",
      "19853/19853 [==============================] - 1s 42us/sample - loss: 5030.0854 - mse: 5030.0854 - mae: 61.8285 - val_loss: 5646.7117 - val_mse: 5646.7114 - val_mae: 49.0343\n",
      "Epoch 16/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 5018.3521 - mse: 5018.3521 - mae: 61.7334 - val_loss: 5635.9773 - val_mse: 5635.9771 - val_mae: 48.9272\n",
      "Epoch 17/70\n",
      "19853/19853 [==============================] - 1s 37us/sample - loss: 5004.8647 - mse: 5004.8647 - mae: 61.6240 - val_loss: 5623.6519 - val_mse: 5623.6528 - val_mae: 48.8039\n",
      "Epoch 18/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 4989.3799 - mse: 4989.3799 - mae: 61.4982 - val_loss: 5609.5259 - val_mse: 5609.5264 - val_mae: 48.6654\n",
      "Epoch 19/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 4971.6270 - mse: 4971.6270 - mae: 61.3535 - val_loss: 5593.3571 - val_mse: 5593.3574 - val_mae: 48.5204\n",
      "Epoch 20/70\n",
      "19853/19853 [==============================] - 1s 37us/sample - loss: 4951.3008 - mse: 4951.3008 - mae: 61.1876 - val_loss: 5574.8881 - val_mse: 5574.8872 - val_mae: 48.3543\n",
      "Epoch 21/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 4928.0718 - mse: 4928.0718 - mae: 60.9974 - val_loss: 5553.8328 - val_mse: 5553.8330 - val_mae: 48.1641\n",
      "Epoch 22/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 4901.5752 - mse: 4901.5752 - mae: 60.7797 - val_loss: 5529.8791 - val_mse: 5529.8789 - val_mae: 47.9467\n",
      "Epoch 23/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 4871.4087 - mse: 4871.4087 - mae: 60.5308 - val_loss: 5502.6878 - val_mse: 5502.6865 - val_mae: 47.7109\n",
      "Epoch 24/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 4837.1367 - mse: 4837.1367 - mae: 60.2469 - val_loss: 5471.8911 - val_mse: 5471.8926 - val_mae: 47.4579\n",
      "Epoch 25/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 4798.2788 - mse: 4798.2788 - mae: 59.9237 - val_loss: 5437.0939 - val_mse: 5437.0942 - val_mae: 47.1699\n",
      "Epoch 26/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 4754.3193 - mse: 4754.3193 - mae: 59.5558 - val_loss: 5397.8763 - val_mse: 5397.8760 - val_mae: 46.8574\n",
      "Epoch 27/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 4704.7012 - mse: 4704.7012 - mae: 59.1384 - val_loss: 5353.7967 - val_mse: 5353.7964 - val_mae: 46.5141\n",
      "Epoch 28/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 4648.8325 - mse: 4648.8325 - mae: 58.6654 - val_loss: 5304.3973 - val_mse: 5304.3965 - val_mae: 46.1455\n",
      "Epoch 29/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 4586.0903 - mse: 4586.0903 - mae: 58.1309 - val_loss: 5249.2094 - val_mse: 5249.2090 - val_mae: 45.7556\n",
      "Epoch 30/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 4515.8237 - mse: 4515.8237 - mae: 57.5285 - val_loss: 5187.7659 - val_mse: 5187.7661 - val_mae: 45.3554\n",
      "Epoch 31/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 4437.3657 - mse: 4437.3657 - mae: 56.8525 - val_loss: 5119.5956 - val_mse: 5119.5957 - val_mae: 44.9345\n",
      "Epoch 32/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 4350.0200 - mse: 4350.0200 - mae: 56.0942 - val_loss: 5044.1468 - val_mse: 5044.1465 - val_mae: 44.4975\n",
      "Epoch 33/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 4252.9648 - mse: 4252.9648 - mae: 55.2462 - val_loss: 4961.0687 - val_mse: 4961.0684 - val_mae: 44.0677\n",
      "Epoch 34/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 4145.5850 - mse: 4145.5850 - mae: 54.3061 - val_loss: 4870.0259 - val_mse: 4870.0264 - val_mae: 43.6498\n",
      "Epoch 35/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 4027.2507 - mse: 4027.2507 - mae: 53.2739 - val_loss: 4770.8106 - val_mse: 4770.8110 - val_mae: 43.2463\n",
      "Epoch 36/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 3897.4358 - mse: 3897.4358 - mae: 52.1419 - val_loss: 4663.3556 - val_mse: 4663.3555 - val_mae: 42.8724\n",
      "Epoch 37/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 3755.7241 - mse: 3755.7241 - mae: 50.9031 - val_loss: 4547.7945 - val_mse: 4547.7944 - val_mae: 42.5215\n",
      "Epoch 38/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 3601.8682 - mse: 3601.8682 - mae: 49.5609 - val_loss: 4424.4935 - val_mse: 4424.4937 - val_mae: 42.1975\n",
      "Epoch 39/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 3435.8206 - mse: 3435.8206 - mae: 48.1206 - val_loss: 4294.1879 - val_mse: 4294.1875 - val_mae: 41.9117\n",
      "Epoch 40/70\n",
      "19853/19853 [==============================] - 1s 39us/sample - loss: 3257.8889 - mse: 3257.8889 - mae: 46.6002 - val_loss: 4158.0247 - val_mse: 4158.0239 - val_mae: 41.6878\n",
      "Epoch 41/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 3068.7585 - mse: 3068.7585 - mae: 45.0074 - val_loss: 4017.7141 - val_mse: 4017.7139 - val_mae: 41.5252\n",
      "Epoch 42/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 2869.6416 - mse: 2869.6416 - mae: 43.2961 - val_loss: 3875.6464 - val_mse: 3875.6455 - val_mae: 41.4284\n",
      "Epoch 43/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 2662.4104 - mse: 2662.4104 - mae: 41.4609 - val_loss: 3735.0146 - val_mse: 3735.0149 - val_mae: 41.4231\n",
      "Epoch 44/70\n",
      "19853/19853 [==============================] - 1s 38us/sample - loss: 2449.7429 - mse: 2449.7429 - mae: 39.5727 - val_loss: 3599.9649 - val_mse: 3599.9648 - val_mae: 41.5400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/70\n",
      "19853/19853 [==============================] - 5s 246us/sample - loss: 2235.2832 - mse: 2235.2832 - mae: 37.8789 - val_loss: 3475.7419 - val_mse: 3475.7422 - val_mae: 41.8088\n",
      "Epoch 46/70\n",
      "19853/19853 [==============================] - 5s 250us/sample - loss: 2023.8163 - mse: 2023.8163 - mae: 36.6597 - val_loss: 3368.7457 - val_mse: 3368.7463 - val_mae: 42.2561\n",
      "Epoch 47/70\n",
      "19853/19853 [==============================] - 5s 265us/sample - loss: 1821.3646 - mse: 1821.3646 - mae: 35.6645 - val_loss: 3286.6443 - val_mse: 3286.6448 - val_mae: 42.9204\n",
      "Epoch 48/70\n",
      "19853/19853 [==============================] - 5s 252us/sample - loss: 1635.3727 - mse: 1635.3727 - mae: 34.7728 - val_loss: 3238.0886 - val_mse: 3238.0881 - val_mae: 43.8650\n",
      "Epoch 49/70\n",
      "19853/19853 [==============================] - 5s 240us/sample - loss: 1474.6871 - mse: 1474.6871 - mae: 33.9166 - val_loss: 3232.0019 - val_mse: 3232.0017 - val_mae: 45.1370\n",
      "Epoch 50/70\n",
      "19853/19853 [==============================] - 5s 236us/sample - loss: 1348.9950 - mse: 1348.9950 - mae: 33.0887 - val_loss: 3276.0663 - val_mse: 3276.0671 - val_mae: 46.7491\n",
      "Epoch 51/70\n",
      "19853/19853 [==============================] - 5s 277us/sample - loss: 1267.7234 - mse: 1267.7234 - mae: 32.4589 - val_loss: 3373.8903 - val_mse: 3373.8899 - val_mae: 48.6413\n",
      "Epoch 52/70\n",
      "19853/19853 [==============================] - 5s 243us/sample - loss: 1237.7664 - mse: 1237.7664 - mae: 32.3122 - val_loss: 3520.6376 - val_mse: 3520.6379 - val_mae: 50.6922\n",
      "Epoch 53/70\n",
      "19853/19853 [==============================] - 6s 295us/sample - loss: 1259.7482 - mse: 1259.7482 - mae: 32.4311 - val_loss: 3698.3629 - val_mse: 3698.3630 - val_mae: 52.7352\n",
      "Epoch 54/70\n",
      "19853/19853 [==============================] - 5s 237us/sample - loss: 1323.6246 - mse: 1323.6246 - mae: 32.6747 - val_loss: 3875.8417 - val_mse: 3875.8418 - val_mae: 54.5394\n",
      "Epoch 55/70\n",
      "19853/19853 [==============================] - 5s 230us/sample - loss: 1407.3009 - mse: 1407.3009 - mae: 33.0793 - val_loss: 4017.4579 - val_mse: 4017.4583 - val_mae: 55.8697\n",
      "Epoch 56/70\n",
      "19853/19853 [==============================] - 5s 227us/sample - loss: 1482.7728 - mse: 1482.7728 - mae: 33.4960 - val_loss: 4097.2331 - val_mse: 4097.2329 - val_mae: 56.5863\n",
      "Epoch 57/70\n",
      "19853/19853 [==============================] - 5s 239us/sample - loss: 1527.6095 - mse: 1527.6095 - mae: 33.7491 - val_loss: 4107.4788 - val_mse: 4107.4790 - val_mae: 56.6765\n",
      "Epoch 58/70\n",
      "19853/19853 [==============================] - 4s 223us/sample - loss: 1533.0388 - mse: 1533.0388 - mae: 33.7755 - val_loss: 4057.0960 - val_mse: 4057.0962 - val_mae: 56.2273\n",
      "Epoch 59/70\n",
      "19853/19853 [==============================] - 5s 251us/sample - loss: 1503.6030 - mse: 1503.6030 - mae: 33.5998 - val_loss: 3964.1766 - val_mse: 3964.1765 - val_mae: 55.3766\n",
      "Epoch 60/70\n",
      "19853/19853 [==============================] - 5s 250us/sample - loss: 1451.5432 - mse: 1451.5432 - mae: 33.3022 - val_loss: 3848.7887 - val_mse: 3848.7888 - val_mae: 54.2723\n",
      "Epoch 61/70\n",
      "19853/19853 [==============================] - 5s 239us/sample - loss: 1390.8591 - mse: 1390.8591 - mae: 32.9704 - val_loss: 3728.3445 - val_mse: 3728.3450 - val_mae: 53.0486\n",
      "Epoch 62/70\n",
      "19853/19853 [==============================] - 5s 269us/sample - loss: 1333.3645 - mse: 1333.3645 - mae: 32.6955 - val_loss: 3615.4972 - val_mse: 3615.4978 - val_mae: 51.8152\n",
      "Epoch 63/70\n",
      "19853/19853 [==============================] - 5s 236us/sample - loss: 1286.8966 - mse: 1286.8966 - mae: 32.5043 - val_loss: 3517.8160 - val_mse: 3517.8162 - val_mae: 50.6483\n",
      "Epoch 64/70\n",
      "19853/19853 [==============================] - 5s 233us/sample - loss: 1255.1024 - mse: 1255.1024 - mae: 32.3845 - val_loss: 3438.5303 - val_mse: 3438.5303 - val_mae: 49.5958\n",
      "Epoch 65/70\n",
      "19853/19853 [==============================] - 4s 223us/sample - loss: 1238.1554 - mse: 1238.1554 - mae: 32.3062 - val_loss: 3377.5388 - val_mse: 3377.5383 - val_mae: 48.6828\n",
      "Epoch 66/70\n",
      "19853/19853 [==============================] - 5s 237us/sample - loss: 1233.8510 - mse: 1233.8510 - mae: 32.2680 - val_loss: 3332.8563 - val_mse: 3332.8564 - val_mae: 47.9180\n",
      "Epoch 67/70\n",
      "19853/19853 [==============================] - 4s 216us/sample - loss: 1238.7738 - mse: 1238.7738 - mae: 32.2667 - val_loss: 3301.5843 - val_mse: 3301.5840 - val_mae: 47.2989\n",
      "Epoch 68/70\n",
      "19853/19853 [==============================] - 4s 227us/sample - loss: 1249.2030 - mse: 1249.2030 - mae: 32.3132 - val_loss: 3280.6651 - val_mse: 3280.6660 - val_mae: 46.8191\n",
      "Epoch 69/70\n",
      "19853/19853 [==============================] - 5s 237us/sample - loss: 1261.7394 - mse: 1261.7394 - mae: 32.4071 - val_loss: 3267.3615 - val_mse: 3267.3611 - val_mae: 46.4680\n",
      "Epoch 70/70\n",
      "19853/19853 [==============================] - 5s 243us/sample - loss: 1273.6582 - mse: 1273.6582 - mae: 32.4995 - val_loss: 3259.4970 - val_mse: 3259.4968 - val_mae: 46.2327\n"
     ]
    }
   ],
   "source": [
    "epochs = 70\n",
    "batch_size = 20000\n",
    "tain_model(model,epochs,batch_size,scaler_x,scaler_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "save_path = 'D:\\\\BUREAU\\\\corsESGI\\\\Projet_annuel\\\\'\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(save_path,\"mpg_model.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(os.path.join(save_path,\"mpg_model-weights.h5\"))\n",
    "model.save('D:\\\\BUREAU\\\\corsESGI\\\\Projet_annuel\\\\Last_trained.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predict on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path= 'D:\\\\BUREAU\\\\corsESGI\\\\Projet_annuel\\\\new_traintest_dataset\\\\Train\\\\'\n",
    "test_path= 'D:\\\\BUREAU\\\\corsESGI\\\\Projet_annuel\\\\new_traintest_dataset\\\\test\\\\'\n",
    "\n",
    "x_test_list = [f for f in listdir( test_path+X_data_path) if isfile(join(test_path+X_data_path, f))]\n",
    "y_test_list =  [f for f in listdir( test_path+target_path) if isfile(join(test_path+target_path, f))]\n",
    "X_test = pd.read_csv(test_path+X_data_path+x_test_list[255],sep=';')\n",
    "Y_test = pd.read_csv(test_path+target_path+y_test_list[555],sep=';',header = None)\n",
    "\n",
    "# .values / np.linalg.norm(X_test[:10].values)\n",
    "pred = X_test[:10]  \n",
    "pred = scaler_x.transform(pred)\n",
    "y_pred = model.predict(pred)\n",
    "scaler_y.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "don2=[[48.830383,2.314821,0,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.829697,2.3162,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.82884,2.31826,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.827895,2.320431,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.83001,2.320966,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831397,2.31646,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832144,2.313393,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.828408,2.32414,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.828357,2.323284,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.826891,2.324099,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.827248,2.322205,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.825948,2.320283,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.828932,2.321381,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.830564,2.316639,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831447,2.322642,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.8319,2.321463,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.828436,2.316733,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832799,2.317827,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.824426,2.320629,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.823743,2.320109,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.827499,2.323774,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.827925,2.322014,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.825821,2.32264,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.825106,2.322596,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.833026,2.315063,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832546,2.314391,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.829688,2.319648,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.83370827,2.32153012,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.833268,2.320961,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832051,2.320206,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831161,2.319783,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.830007,2.318341,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.829556,2.317668,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.828919,2.316764,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.828363,2.315922,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831071,2.322159,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832633,2.321934,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.833705,2.316678,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.829915,2.319076,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.824523,2.316429,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.825239,2.314221,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.824709,2.315544,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832111,2.323676,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.833574,2.315312,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832591,2.316675,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831675,2.317922,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.830908,2.319052,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.826294,2.31921,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.826647,2.317179,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.824865,2.313759,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832863,2.32282,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831522,2.321805,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831034,2.321137,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.829423,2.318896,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.829787,2.317505,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.82719,2.325484,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.826599,2.323751,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.826034,2.322872,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.825628,2.321953,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.825219,2.321017,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.824811,2.320042,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.828757,2.317529,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.82994,2.31469,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.829403,2.315588,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.82954,2.321017,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.827233,2.316016,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.8307,2.323003,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.828691,2.320577,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831125,2.325428,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.830536,2.325864,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.829538,2.326301,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.824095,2.315431,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.833412,2.322577,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.824607,2.322069,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.829349,2.325026,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.829792,2.323661,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.83111,2.320253,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831495,2.319373,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831851,2.318676,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.823947,2.319435,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831453,2.320761,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.83307213,2.31417149,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832554,2.31319,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832844,2.319992,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.833571,2.318876,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.828501,2.315036,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.829553,2.313484,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.830419,2.312035,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.83263018,2.31955024,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.83335139,2.31825749,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831889,2.324515,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.830835,2.32377,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.826805,2.32029,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.833139,2.316733,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832094,2.314725,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.830957,2.312533,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.830373,2.311285,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831489,2.311521,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832068,2.32256,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832114,2.321769,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.832382,2.323734,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831109,2.314715,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.830334,2.313574,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831974,2.316754,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[48.831307,2.317388,3,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]\n",
    "norm_mer=scaler_x.transform(np.array(don2))\n",
    "# scaler_y.inverse_transform(model.predict(norm_mer))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
